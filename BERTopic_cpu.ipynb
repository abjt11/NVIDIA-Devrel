{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4c08df7",
   "metadata": {},
   "source": [
    "# Showcase BERTopic in CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce380d11-ca1f-4a71-b181-05f0eed80988",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Installing BERTopic and other necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29ce7bc6-4382-4683-9051-07d9ad9cab2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.12.0\n",
      "  latest version: 4.13.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ubuntu/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - hdbscan\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    conda-4.12.0               |   py39hf3d152e_0        1014 KB  conda-forge\n",
      "    hdbscan-0.8.27             |   py39hce5d2b2_0         695 KB  conda-forge\n",
      "    python_abi-3.9             |           2_cp39           4 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         1.7 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  hdbscan            conda-forge/linux-64::hdbscan-0.8.27-py39hce5d2b2_0\n",
      "  python_abi         conda-forge/linux-64::python_abi-3.9-2_cp39\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  conda              pkgs/main::conda-4.12.0-py39h06a4308_0 --> conda-forge::conda-4.12.0-py39hf3d152e_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "python_abi-3.9       | 4 KB      | ##################################### | 100% \n",
      "hdbscan-0.8.27       | 695 KB    | ##################################### | 100% \n",
      "conda-4.12.0         | 1014 KB   | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda install -c conda-forge hdbscan -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44b814c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ubuntu/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - conda\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    conda-4.13.0               |   py39h06a4308_0         895 KB\n",
      "    conda-repo-cli-1.0.5       |   py39h06a4308_0         107 KB\n",
      "    pathlib-1.0.1              |     pyhd3eb1b0_1          17 KB\n",
      "    pyjwt-2.4.0                |   py39h06a4308_0          34 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         1.0 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  pathlib            pkgs/main/noarch::pathlib-1.0.1-pyhd3eb1b0_1\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  conda              conda-forge::conda-4.12.0-py39hf3d152~ --> pkgs/main::conda-4.13.0-py39h06a4308_0\n",
      "  conda-repo-cli     pkgs/main/noarch::conda-repo-cli-1.0.~ --> pkgs/main/linux-64::conda-repo-cli-1.0.5-py39h06a4308_0\n",
      "  pyjwt                                2.1.0-py39h06a4308_0 --> 2.4.0-py39h06a4308_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "conda-repo-cli-1.0.5 | 107 KB    | ##################################### | 100% \n",
      "pathlib-1.0.1        | 17 KB     | ##################################### | 100% \n",
      "conda-4.13.0         | 895 KB    | ##################################### | 100% \n",
      "pyjwt-2.4.0          | 34 KB     | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda update -n base conda -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "472bbb0c-b542-41ab-adee-d94f29c0f374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/ubuntu/anaconda3/lib/python3.9/site-packages (21.2.4)\n",
      "Collecting pip\n",
      "  Using cached pip-22.2.2-py3-none-any.whl (2.0 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 21.2.4\n",
      "    Uninstalling pip-21.2.4:\n",
      "      Successfully uninstalled pip-21.2.4\n",
      "Successfully installed pip-22.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "389becf0-f030-476e-b92d-b5af8e0c1981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hdbscan in /home/ubuntu/anaconda3/lib/python3.9/site-packages (0.8.27)\n",
      "Requirement already satisfied: numpy>=1.16 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from hdbscan) (1.21.5)\n",
      "Requirement already satisfied: scipy>=1.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from hdbscan) (1.7.3)\n",
      "Requirement already satisfied: cython>=0.27 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from hdbscan) (0.29.28)\n",
      "Requirement already satisfied: joblib>=1.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from hdbscan) (1.1.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from hdbscan) (1.0.2)\n",
      "Requirement already satisfied: six in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from hdbscan) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from scikit-learn>=0.20->hdbscan) (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install hdbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7df5b2e8-f3ac-47c2-b884-38886012c879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /home/ubuntu/anaconda3/lib/python3.9/site-packages (22.2.2)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/anaconda3/lib/python3.9/site-packages (65.0.0)\n",
      "Requirement already satisfied: wheel in /home/ubuntu/anaconda3/lib/python3.9/site-packages (0.37.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip setuptools wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca6cbc8d-55b0-4c25-8cd2-ee7411fdd978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bertopic\n",
      "  Using cached bertopic-0.11.0-py2.py3-none-any.whl (76 kB)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from bertopic) (4.64.0)\n",
      "Collecting sentence-transformers>=0.4.1\n",
      "  Using cached sentence_transformers-2.2.2-py3-none-any.whl\n",
      "Requirement already satisfied: numpy>=1.20.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from bertopic) (1.21.5)\n",
      "Collecting umap-learn>=0.5.0\n",
      "  Using cached umap_learn-0.5.3-py3-none-any.whl\n",
      "Collecting pyyaml<6.0\n",
      "  Using cached PyYAML-5.4.1-cp39-cp39-manylinux1_x86_64.whl (630 kB)\n",
      "Collecting hdbscan>=0.8.28\n",
      "  Using cached hdbscan-0.8.28.tar.gz (5.2 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22.2.post1 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from bertopic) (1.0.2)\n",
      "Requirement already satisfied: plotly>=4.7.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from bertopic) (5.6.0)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from bertopic) (1.4.2)\n",
      "Requirement already satisfied: joblib>=1.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from hdbscan>=0.8.28->bertopic) (1.1.0)\n",
      "Requirement already satisfied: cython>=0.27 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from hdbscan>=0.8.28->bertopic) (0.29.28)\n",
      "Requirement already satisfied: scipy>=1.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from hdbscan>=0.8.28->bertopic) (1.7.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from pandas>=1.1.5->bertopic) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from pandas>=1.1.5->bertopic) (2021.3)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from plotly>=4.7.0->bertopic) (8.0.1)\n",
      "Requirement already satisfied: six in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from plotly>=4.7.0->bertopic) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from scikit-learn>=0.22.2.post1->bertopic) (2.2.0)\n",
      "Requirement already satisfied: nltk in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from sentence-transformers>=0.4.1->bertopic) (3.7)\n",
      "Collecting torch>=1.6.0\n",
      "  Using cached torch-1.12.1-cp39-cp39-manylinux1_x86_64.whl (776.4 MB)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.13.1-cp39-cp39-manylinux1_x86_64.whl (19.1 MB)\n",
      "Collecting huggingface-hub>=0.4.0\n",
      "  Using cached huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Using cached transformers-4.21.1-py3-none-any.whl (4.7 MB)\n",
      "Collecting pynndescent>=0.5\n",
      "  Using cached pynndescent-0.5.7-py3-none-any.whl\n",
      "Requirement already satisfied: numba>=0.49 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from umap-learn>=0.5.0->bertopic) (0.55.1)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (4.1.1)\n",
      "Requirement already satisfied: requests in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.27.1)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (65.0.0)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from numba>=0.49->umap-learn>=0.5.0->bertopic) (0.38.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers>=0.4.1->bertopic) (2022.3.15)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Using cached tokenizers-0.12.1-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "Requirement already satisfied: click in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from nltk->sentence-transformers>=0.4.1->bertopic) (8.0.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from torchvision->sentence-transformers>=0.4.1->bertopic) (9.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers>=0.4.1->bertopic) (2.0.4)\n",
      "Building wheels for collected packages: hdbscan\n",
      "  Building wheel for hdbscan (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hdbscan: filename=hdbscan-0.8.28-cp39-cp39-linux_x86_64.whl size=826749 sha256=7744ad6b2b326f753c67ce6ca3886f5d06a791b05c87abe88662835f64740f40\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/97/2d/1e/d9907e8f806ee949f9effc41004d7f32e862f6f67d9157812d\n",
      "Successfully built hdbscan\n",
      "Installing collected packages: tokenizers, sentencepiece, torch, pyyaml, torchvision, huggingface-hub, transformers, pynndescent, hdbscan, umap-learn, sentence-transformers, bertopic\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 6.0\n",
      "    Uninstalling PyYAML-6.0:\n",
      "      Successfully uninstalled PyYAML-6.0\n",
      "  Attempting uninstall: hdbscan\n",
      "    Found existing installation: hdbscan 0.8.27\n",
      "    Uninstalling hdbscan-0.8.27:\n",
      "      Successfully uninstalled hdbscan-0.8.27\n",
      "Successfully installed bertopic-0.11.0 hdbscan-0.8.28 huggingface-hub-0.8.1 pynndescent-0.5.7 pyyaml-5.4.1 sentence-transformers-2.2.2 sentencepiece-0.1.97 tokenizers-0.12.1 torch-1.12.1 torchvision-0.13.1 transformers-4.21.1 umap-learn-0.5.3\n"
     ]
    }
   ],
   "source": [
    "!pip install bertopic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "feefffe0-1b77-48e7-8d03-1b6d5377ad8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Needed to access folders\n",
    "# !pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc56072b-c3b7-47c9-add2-16deceab4706",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Needed to access Amazon S3 buckets\n",
    "# !pip install awswrangler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26cf1055-8e61-48f0-8486-b8bc21214054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting apache_beam\n",
      "  Downloading apache_beam-2.40.0-cp39-cp39-manylinux2010_x86_64.whl (12.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting mwparserfromhell\n",
      "  Downloading mwparserfromhell-0.6.4-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (178 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf<4,>=3.12.2 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from apache_beam) (3.19.1)\n",
      "Collecting fastavro<2,>=0.23.6\n",
      "  Downloading fastavro-1.5.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m133.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting orjson<4.0\n",
      "  Downloading orjson-3.7.12-cp39-cp39-manylinux_2_28_x86_64.whl (148 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.1/148.1 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: grpcio<2,>=1.33.1 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from apache_beam) (1.42.0)\n",
      "Collecting hdfs<3.0.0,>=2.1.0\n",
      "  Downloading hdfs-2.7.0-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: numpy<1.23.0,>=1.14.3 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from apache_beam) (1.21.5)\n",
      "Collecting pymongo<4.0.0,>=3.8.0\n",
      "  Downloading pymongo-3.12.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (516 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m516.3/516.3 kB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3,>=2.8.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from apache_beam) (2.8.2)\n",
      "Collecting pyarrow<8.0.0,>=0.15.1\n",
      "  Downloading pyarrow-7.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.2,>=0.3.1.1\n",
      "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.24.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from apache_beam) (2.27.1)\n",
      "Collecting proto-plus<2,>=1.7.1\n",
      "  Downloading proto_plus-1.22.0-py3-none-any.whl (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2018.3 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from apache_beam) (2021.3)\n",
      "Collecting cloudpickle<3,>=2.1.0\n",
      "  Downloading cloudpickle-2.1.0-py3-none-any.whl (25 kB)\n",
      "Collecting pydot<2,>=1.2.0\n",
      "  Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
      "Collecting httplib2<0.21.0,>=0.8\n",
      "  Downloading httplib2-0.20.4-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.6/96.6 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from apache_beam) (4.1.1)\n",
      "Collecting crcmod<2.0,>=1.7\n",
      "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5.2 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from grpcio<2,>=1.33.1->apache_beam) (1.16.0)\n",
      "Collecting docopt\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from httplib2<0.21.0,>=0.8->apache_beam) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.24.0->apache_beam) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.24.0->apache_beam) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.24.0->apache_beam) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.24.0->apache_beam) (3.3)\n",
      "Building wheels for collected packages: crcmod, dill, docopt\n",
      "  Building wheel for crcmod (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for crcmod: filename=crcmod-1.7-cp39-cp39-linux_x86_64.whl size=23649 sha256=9bbfcb736c4ca2fa5ced2ac41bb5ef480ec74789ae6e98094e1f3f7ba954b140\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/4a/6c/a6/ffdd136310039bf226f2707a9a8e6857be7d70a3fc061f6b36\n",
      "  Building wheel for dill (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78544 sha256=775109701b1d63ff5adc95bcc079ca5e42c27523f73cff841f4e3e6a783864c4\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/4f/0b/ce/75d96dd714b15e51cb66db631183ea3844e0c4a6d19741a149\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=8482e4d7d2fc96a416b5b7ed38fd6c65f2972ea918c0f1d26cca9d211b29bf2a\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/70/4a/46/1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
      "Successfully built crcmod dill docopt\n",
      "Installing collected packages: docopt, crcmod, pymongo, pydot, pyarrow, proto-plus, orjson, mwparserfromhell, httplib2, fastavro, dill, cloudpickle, hdfs, apache_beam\n",
      "  Attempting uninstall: cloudpickle\n",
      "    Found existing installation: cloudpickle 2.0.0\n",
      "    Uninstalling cloudpickle-2.0.0:\n",
      "      Successfully uninstalled cloudpickle-2.0.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.1.5 requires pyqt5<5.13, which is not installed.\n",
      "spyder 5.1.5 requires pyqtwebengine<5.13, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed apache_beam-2.40.0 cloudpickle-2.1.0 crcmod-1.7 dill-0.3.1.1 docopt-0.6.2 fastavro-1.5.4 hdfs-2.7.0 httplib2-0.20.4 mwparserfromhell-0.6.4 orjson-3.7.12 proto-plus-1.22.0 pyarrow-7.0.0 pydot-1.4.2 pymongo-3.12.3\n"
     ]
    }
   ],
   "source": [
    "#Needed to access Wikipedia corpus\n",
    "!pip install apache_beam mwparserfromhell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cba1053f-1605-4109-abb6-d47fb0aac4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from datasets) (1.21.5)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from datasets) (2022.2.0)\n",
      "Requirement already satisfied: aiohttp in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: pandas in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from datasets) (1.4.2)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.13-py39-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.3/132.3 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from datasets) (7.0.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from datasets) (2.27.1)\n",
      "Requirement already satisfied: dill<0.3.6 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from datasets) (0.3.1.1)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (211 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.2/211.2 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from datasets) (4.64.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from datasets) (0.8.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (5.4.1)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from packaging->datasets) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (1.6.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (5.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Collecting dill<0.3.6\n",
      "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.8/95.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, dill, responses, multiprocess, datasets\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.1.1\n",
      "    Uninstalling dill-0.3.1.1:\n",
      "      Successfully uninstalled dill-0.3.1.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "apache-beam 2.40.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-2.4.0 dill-0.3.5.1 multiprocess-0.70.13 responses-0.18.0 xxhash-3.0.0\n"
     ]
    }
   ],
   "source": [
    "#Needed to access Wikipedia corpus\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992fa566-80da-435d-b8e5-81a821ae3d72",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2809c2f-66e6-4220-a7a0-b184b2819bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing necessary packages\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "# import rmm\n",
    "import os\n",
    "# import boto3\n",
    "import sys\n",
    "import pandas as pd\n",
    "import csv\n",
    "import io\n",
    "from bertopic import BERTopic\n",
    "from hdbscan import HDBSCAN\n",
    "import numpy as np\n",
    "import time\n",
    "pd.set_option('max_colwidth', -1)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "# rmm.reinitialize(pool_allocator=True,initial_pool_size=5e+9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54618d2e",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b89449a2-a612-4405-9e8a-691efa1a185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing custom datasets for the lab\n",
    "#All the datasets names are stored in a list 'bucket_list' for ease of tracking \n",
    "bucket_list = []\n",
    "\n",
    "#Importing news dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "docs = fetch_20newsgroups(subset='all')['data']\n",
    "bucket_list.append('docs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ae535d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['docs', './data/CUADv1.json', './data/an4_sphere.tar.gz']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code will pickup files from data folder and add it to bucket_list\n",
    "data_folder = \"./data/\"\n",
    "list_files = os.listdir(data_folder)\n",
    "for i in list_files:\n",
    "    if i.find(\".md\")==-1:\n",
    "        bucket_list.append(data_folder+i)\n",
    "bucket_list\n",
    "\n",
    "#Following lines of code help with picking up data files from S3 bucket\n",
    "#Loading data from S3 bucket\n",
    "# s3_client = boto3.client('s3')\n",
    "# s3_bucket_name = 'nvidia-rapids'\n",
    "# s3 = boto3.resource('s3')\n",
    "# my_bucket=s3.Bucket(s3_bucket_name)\n",
    "\n",
    "# for file in my_bucket.objects.filter():\n",
    "#     file_name=file.key\n",
    "#     if file_name!=\"data/\" and file_name.find(\"data/\")!=-1:\n",
    "#         bucket_list.append(file.key)\n",
    "# bucket_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fd87ab7-5504-45a8-882a-8ce2bb8ba030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86318236ca654c15a912988e855c96e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/11.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11d2d3547cda4b7dab9752d9faf9de25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/7.14k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset wikipedia/20220301.en (download: 19.18 GiB, generated: 18.88 GiB, post-processed: Unknown size, total: 38.07 GiB) to /home/ubuntu/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f19bfe32fd0431284b2abb908f82a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/15.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54c98b582b3444178e24317ac7725513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/20.3G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset wikipedia downloaded and prepared to /home/ubuntu/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f629ce24f6ec48dab62c68cf67f9bfde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['docs', './data/CUADv1.json', './data/an4_sphere.tar.gz', 'data_wiki']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing Wikidata corpus\n",
    "from datasets import load_dataset\n",
    "data_wiki = pd.DataFrame(load_dataset(\"wikipedia\", \"20220301.en\"))\n",
    "bucket_list.append('data_wiki')\n",
    "\n",
    "bucket_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45ce4a2a-9a49-4518-a4b5-8123d4effc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to read the data files\n",
    "data_wiki_rows = 1000000  #As Wikipedia data is huge, to ensure a lower runtime, we can update this variable to pickup these many number of rows \n",
    "for i in range(len(bucket_list)):\n",
    "    if bucket_list[i].find(\".json\")!=-1:\n",
    "        globals()[\"data_\" + str(i)] = pd.read_json(bucket_list[i], orient='columns')\n",
    "    elif bucket_list[i].find(\".tar.gz\")!=-1:\n",
    "        globals()[\"data_\" + str(i)] = pd.read_csv(bucket_list[i], compression='gzip', header=None, encoding=\"cp437\", delimiter =';', engine='python', on_bad_lines='skip')\n",
    "        # globals()[\"data_\" + str(i)] = pd.read_csv('s3://'+s3_bucket_name+'/'+bucket_list[i], compression='gzip', header=0, sep=' ', on_bad_lines='skip')\n",
    "    elif bucket_list[i].find(\".pkl\")!=-1:\n",
    "        globals()[\"data_\" + str(i)] = pd.read_pickle(bucket_list[i])\n",
    "    elif bucket_list[i]=='docs':\n",
    "        globals()[\"data_\" + str(i)] = docs\n",
    "    elif bucket_list[i]=='data_wiki':\n",
    "        globals()[\"data_\" + str(i)] = data_wiki.head(data_wiki_rows)\n",
    "    else:\n",
    "        globals()[\"data_\" + str(i)] = pd.read_csv(bucket_list[i])\n",
    "\n",
    "#Following lines of code help with reading data files from S3 bucket\n",
    "# for i in range(len(bucket_list)):\n",
    "#     if bucket_list[i].find(\".json\")!=-1:\n",
    "#         globals()[\"data_\" + str(i)] = pd.read_json('s3://'+s3_bucket_name+'/'+bucket_list[i], orient='columns')\n",
    "#     elif bucket_list[i].find(\".tar.gz\")!=-1:\n",
    "#         globals()[\"data_\" + str(i)] = pd.read_csv('s3://'+s3_bucket_name+'/'+bucket_list[i], compression='gzip', header=None, encoding=\"cp437\", delimiter =';', engine='python', on_bad_lines='skip')\n",
    "#         # globals()[\"data_\" + str(i)] = pd.read_csv('s3://'+s3_bucket_name+'/'+bucket_list[i], compression='gzip', header=0, sep=' ', on_bad_lines='skip')\n",
    "#     elif bucket_list[i].find(\".pkl\")!=-1:\n",
    "#         globals()[\"data_\" + str(i)] = pd.read_pickle('s3://'+s3_bucket_name+'/'+bucket_list[i])\n",
    "#     elif bucket_list[i]=='docs':\n",
    "#         globals()[\"data_\" + str(i)] = docs\n",
    "#     elif bucket_list[i]=='data_wiki':\n",
    "#         globals()[\"data_\" + str(i)] = data_wiki\n",
    "#     else:\n",
    "#         globals()[\"data_\" + str(i)] = pd.read_csv('s3://'+s3_bucket_name+'/'+bucket_list[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb29575b",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62a59486-feea-4944-8f35-b66417a25529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to vectorize dataset 1(in s): 5.16\n",
      "Time to vectorize dataset 2(in s): 5.45\n",
      "Time to vectorize dataset 3(in s): 0.57\n",
      "Time to vectorize dataset 4(in s): 956.97\n"
     ]
    }
   ],
   "source": [
    "#Data Pre-processing step\n",
    "#Sample Text cleaning before fitting into model\n",
    "#Most NLP corpus needs customized cleaning based on datasets\n",
    "#But for the purpose of demonstration, we are only keeps words as topics and removing numbers and other special characters\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "stop=stopwords.words('english')\n",
    "pat1=re.compile(r\"[^a-zA-Z ]+\")\n",
    "pat2=re.compile(r'\\b(?:{})\\b'.format('|'.join(stop)))\n",
    "\n",
    "#Data cleaning for various datasets\n",
    "start_time = time.time()\n",
    "data_0 = pd.DataFrame(data_0)\n",
    "data_0.columns = ['train']\n",
    "data_0 = data_0.train.astype(str).str.replace(pat1,\" \").str.replace(pat2,\" \").str.strip()\n",
    "end_time = time.time() - start_time\n",
    "print(\"Time to vectorize dataset 1(in s): \"+ str(np.round(end_time, decimals=2)))\n",
    "\n",
    "start_time = time.time()\n",
    "data_1 = data_1.data.astype(str).str.replace(pat1,\" \").str.replace(pat2,\" \").str.strip()\n",
    "end_time = time.time() - start_time\n",
    "print(\"Time to vectorize dataset 2(in s): \"+ str(np.round(end_time, decimals=2)))\n",
    "\n",
    "start_time = time.time()\n",
    "data_2 = data_2[0].astype(str).str.replace(pat1,\" \").str.replace(pat2,\" \").str.strip()\n",
    "end_time = time.time() - start_time\n",
    "print(\"Time to vectorize dataset 3(in s): \"+ str(np.round(end_time, decimals=2)))\n",
    "\n",
    "start_time = time.time()\n",
    "data_3 = data_3.train.astype(str).str.replace(pat1,\" \").str.replace(pat2,\" \").str.strip()\n",
    "end_time = time.time() - start_time\n",
    "print(\"Time to vectorize dataset 4(in s): \"+ str(np.round(end_time, decimals=2)))\n",
    "\n",
    "# def random_function(x):\n",
    "#     return ((str(x).replace(r\"[^a-zA-Z ]+\", \" \")).strip())\n",
    "# data_0 = np.vectorize(random_function)(data_0.data) \n",
    "# data_1 = np.vectorize(random_function)(data_1[0].head(20000))\n",
    "# data_3=data_3.head(10000)\n",
    "# data_0 = data_0.data.apply(str).str.replace(r\"[^a-zA-Z ]+\", \" \").str.strip()\n",
    "# data_3=data_3['train'].astype(str).str.replace(r\"[^a-zA-Z ]+\", \" \").str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e21d82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset0 info: \n",
      "<class 'pandas.core.series.Series'>\n",
      "Int64Index: 18846 entries, 0 to 18845\n",
      "Series name: train\n",
      "Non-Null Count  Dtype \n",
      "--------------  ----- \n",
      "18846 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 294.5+ KB\n",
      "None\n",
      "Size: 30896294\n",
      "Memory: 301536\n",
      "\n",
      "---------\n",
      "Dataset1 info: \n",
      "<class 'pandas.core.series.Series'>\n",
      "Int64Index: 510 entries, 0 to 509\n",
      "Series name: data\n",
      "Non-Null Count  Dtype \n",
      "--------------  ----- \n",
      "510 non-null    object\n",
      "dtypes: object(1)\n",
      "memory usage: 8.0+ KB\n",
      "None\n",
      "Size: 34454328\n",
      "Memory: 8160\n",
      "\n",
      "---------\n",
      "Dataset2 info: \n",
      "<class 'pandas.core.series.Series'>\n",
      "Int64Index: 227143 entries, 0 to 227142\n",
      "Series name: 0\n",
      "Non-Null Count   Dtype \n",
      "--------------   ----- \n",
      "227143 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 3.5+ MB\n",
      "None\n",
      "Size: 15418194\n",
      "Memory: 3634288\n",
      "\n",
      "---------\n",
      "Dataset3 info: \n",
      "<class 'pandas.core.series.Series'>\n",
      "Int64Index: 1000000 entries, 0 to 999999\n",
      "Series name: train\n",
      "Non-Null Count    Dtype \n",
      "--------------    ----- \n",
      "1000000 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 15.3+ MB\n",
      "None\n",
      "Size: 5808312755\n",
      "Memory: 16000000\n",
      "\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "#Storing pre-procssed data into a folder to prevent the need of data pre-processing in future iterations\n",
    "processed_data_folder = \"./processed_data/\"\n",
    "for i in range(len(bucket_list)):\n",
    "    print(\"Dataset\"+str(i)+\" info: \")\n",
    "    globals()[\"data_\" + str(i)] = globals()[\"data_\" + str(i)].dropna()\n",
    "    print(globals()[\"data_\" + str(i)].info())\n",
    "    print(\"Size: \"+str(sys.getsizeof(globals()[\"data_\" + str(i)])))\n",
    "    print(\"Memory: \"+str((globals()[\"data_\" + str(i)]).memory_usage(index=True)))\n",
    "    print()\n",
    "    print(\"---------\")\n",
    "    globals()[\"data_\" + str(i)].to_csv(processed_data_folder+\"data_\"+str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3f32a1",
   "metadata": {},
   "source": [
    "# Topic Modeling using BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "786d4910-cc16-44e1-8847-95ef19f4f518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug 15 06:58:34 2022\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bfafff5af8d4bcc9d1a423d41c51614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e10fcae7ba34d2b964a8620fc87035e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ec16a638f5a405685208bb3aa6ee7fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace79ee2089947748389ae396d3010ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f07ff884f834f5e8b139ef9f2e4ca34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e94bfe01ba7346e6a148c2feee8bb474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea92b0239f764135b70d8ccc25b8e5d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36b9d9669d3c46c78900f51409f59315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c08707259154d6bac931370d149c567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a33e75bdaf4e11ae0be4a883f0d929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "871e585f2c1e4dc2a014c53c0e216811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26269336224940d6916af890c55cddb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e288bf297f0942d496ce79e4540bf053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e124a63105c94ab1b707c5a238dfed86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aafe58752f544778fad109ad4aa6733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/589 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-15 07:01:16,851 - BERTopic - Transformed documents to Embeddings\n",
      "2022-08-15 07:01:37,088 - BERTopic - Reduced dimensionality\n",
      "2022-08-15 07:01:40,404 - BERTopic - Clustered reduced embeddings\n",
      "2022-08-15 07:01:58,424 - BERTopic - Reduced number of topics from 295 to 236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to perform topic modeling for dataset1(in s): 204.01\n",
      "Mon Aug 15 07:04:54 2022\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17a514a032be4f6090bd44509b4d99e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-15 07:05:05,580 - BERTopic - Transformed documents to Embeddings\n",
      "2022-08-15 07:05:07,816 - BERTopic - Reduced dimensionality\n",
      "2022-08-15 07:05:07,834 - BERTopic - Clustered reduced embeddings\n",
      "2022-08-15 07:05:15,570 - BERTopic - Reduced number of topics from 14 to 14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to perform topic modeling for dataset2(in s): 20.96\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(bucket_list)):\n",
    "    if i>1:   #Counter to run specific datasets\n",
    "        continue\n",
    "    globals()[\"start_time_\" + str(i)] = time.time()\n",
    "    print(time.ctime())\n",
    "    globals()[\"topic_model_cpu_\" + str(i)] = BERTopic(verbose=True, nr_topics=\"auto\",)\n",
    "    globals()[\"topics_\" + str(i)], globals()[\"probs_\" + str(i)] = globals()[\"topic_model_cpu_\" + str(i)].fit_transform(globals()[\"data_\" + str(i)])\n",
    "    globals()[\"end_time_cpu_\" + str(i)] = time.time() - globals()[\"start_time_\" + str(i)]\n",
    "    print(\"Time to perform topic modeling for dataset\" +str(i+1) + \"(in s): \"+ str(np.round(globals()[\"end_time_cpu_\" + str(i)], decimals=2)))\n",
    "    \n",
    "    viz_topics = globals()[\"topic_model_cpu_\" + str(i)].visualize_topics()\n",
    "    # print(viz_topics.show())\n",
    "    viz_topics.write_html(\"results/viz_topics_cpu_\"+str(i)+\".html\")\n",
    "    viz_barchart = globals()[\"topic_model_cpu_\" + str(i)].visualize_barchart()\n",
    "    # print(viz_barchart.show())\n",
    "    viz_barchart.write_html(\"results/viz_barchart_cpu_\"+str(i)+\".html\")\n",
    "    viz_docs = globals()[\"topic_model_cpu_\" + str(i)].visualize_documents(globals()[\"data_\" + str(i)])\n",
    "    viz_docs.write_html(\"results/viz_docs_cpu_\"+str(i)+\".html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "503faeb8-a6b9-4110-b568-f9778d7f7d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug 15 07:06:11 2022\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85aca9c55202486fad3745d4dded6763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/7099 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-15 07:10:21,969 - BERTopic - Transformed documents to Embeddings\n",
      "2022-08-15 07:49:31,586 - BERTopic - Reduced dimensionality\n",
      "2022-08-15 07:49:56,555 - BERTopic - Clustered reduced embeddings\n",
      "2022-08-15 07:50:11,669 - BERTopic - Reduced number of topics from 1147 to 473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to perform topic modeling for dataset3(in s): 2639.73\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(bucket_list)):\n",
    "    if i!=2:   #Counter to run specific datasets\n",
    "        continue\n",
    "    globals()[\"start_time_\" + str(i)] = time.time()\n",
    "    print(time.ctime())\n",
    "    globals()[\"topic_model_cpu_\" + str(i)] = BERTopic(verbose=True, nr_topics=\"auto\",)\n",
    "    globals()[\"topics_\" + str(i)], globals()[\"probs_\" + str(i)] = globals()[\"topic_model_cpu_\" + str(i)].fit_transform(globals()[\"data_\" + str(i)])\n",
    "    globals()[\"end_time_cpu_\" + str(i)] = time.time() - globals()[\"start_time_\" + str(i)]\n",
    "    print(\"Time to perform topic modeling for dataset\" +str(i+1) + \"(in s): \"+ str(np.round(globals()[\"end_time_cpu_\" + str(i)], decimals=2)))\n",
    "    \n",
    "    # viz_topics = globals()[\"topic_model_cpu_\" + str(i)].visualize_topics()\n",
    "    # # print(viz_topics.show())\n",
    "    # viz_topics.write_html(\"results/viz_topics_cpu_\"+str(i)+\".html\")\n",
    "    # viz_barchart = globals()[\"topic_model_cpu_\" + str(i)].visualize_barchart()\n",
    "    # # print(viz_barchart.show())\n",
    "    # viz_barchart.write_html(\"results/viz_barchart_cpu_\"+str(i)+\".html\")\n",
    "    # viz_docs = globals()[\"topic_model_cpu_\" + str(i)].visualize_documents(globals()[\"data_\" + str(i)])\n",
    "    # viz_docs.write_html(\"results/viz_docs_cpu_\"+str(i)+\".html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3517e44f-442a-4443-bad6-c0423ad18b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>7084</td>\n",
       "      <td>-1_edu_com_the_subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1270</td>\n",
       "      <td>0_game_team_games_baseball</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>771</td>\n",
       "      <td>1_key_clipper_encryption_chip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>353</td>\n",
       "      <td>2_god_jesus_christ_bible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>339</td>\n",
       "      <td>3_israel_israeli_arab_jews</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>230</td>\n",
       "      <td>11</td>\n",
       "      <td>230_rh_liar_lunatic_bissell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>231</td>\n",
       "      <td>10</td>\n",
       "      <td>231_land_property_wage_kids</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>232</td>\n",
       "      <td>10</td>\n",
       "      <td>232_xvertext_sussex_mppa_syma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>233</td>\n",
       "      <td>10</td>\n",
       "      <td>233_convex_visser_pubic_hairs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>234</td>\n",
       "      <td>10</td>\n",
       "      <td>234_cd_apple_cdrom_applelink</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>236 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic  Count                           Name\n",
       "0   -1      7084   -1_edu_com_the_subject       \n",
       "1    0      1270   0_game_team_games_baseball   \n",
       "2    1      771    1_key_clipper_encryption_chip\n",
       "3    2      353    2_god_jesus_christ_bible     \n",
       "4    3      339    3_israel_israeli_arab_jews   \n",
       "..  ..      ...                           ...   \n",
       "231  230    11     230_rh_liar_lunatic_bissell  \n",
       "232  231    10     231_land_property_wage_kids  \n",
       "233  232    10     232_xvertext_sussex_mppa_syma\n",
       "234  233    10     233_convex_visser_pubic_hairs\n",
       "235  234    10     234_cd_apple_cdrom_applelink \n",
       "\n",
       "[236 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model_cpu_0.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c69e9a24-fea9-4aa8-8f39-d1c5c2f98fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('game', 0.012488819768867251),\n",
       " ('team', 0.010677236700507593),\n",
       " ('games', 0.009776389054572182),\n",
       " ('baseball', 0.008980899273839325),\n",
       " ('year', 0.00831041850235353),\n",
       " ('season', 0.007813869938817934),\n",
       " ('players', 0.007424436509869103),\n",
       " ('league', 0.006786845223771858),\n",
       " ('hockey', 0.006397511235252266),\n",
       " ('hit', 0.0058796397520957085)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model_cpu_0.get_topic(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87c188c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>142</td>\n",
       "      <td>-1_agreement_party_shall_contract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>0_party_agreement_shall_company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>1_distributor_agreement_party_ex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>2_shall_party_agreement_product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>3_sponsorship_sponsor_agreement_contract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "      <td>4_maintenance_agreement_shall_contract</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>5_endorsement_contract_agreement_ex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>6_co_verticalnet_branding_party</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>7_hosting_customer_software_agreement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>8_msl_ibm_customer_outsourcing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>9_spinco_group_intellectual_property</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>10_transportation_shipper_transporter_shall</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>11_franchise_franchisee_us_pretzel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>12_reseller_agreement_ehave_touchstar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count                                         Name\n",
       "0  -1      142    -1_agreement_party_shall_contract          \n",
       "1   0      67     0_party_agreement_shall_company            \n",
       "2   1      61     1_distributor_agreement_party_ex           \n",
       "3   2      38     2_shall_party_agreement_product            \n",
       "4   3      32     3_sponsorship_sponsor_agreement_contract   \n",
       "5   4      31     4_maintenance_agreement_shall_contract     \n",
       "6   5      22     5_endorsement_contract_agreement_ex        \n",
       "7   6      21     6_co_verticalnet_branding_party            \n",
       "8   7      20     7_hosting_customer_software_agreement      \n",
       "9   8      17     8_msl_ibm_customer_outsourcing             \n",
       "10  9      16     9_spinco_group_intellectual_property       \n",
       "11  10     15     10_transportation_shipper_transporter_shall\n",
       "12  11     15     11_franchise_franchisee_us_pretzel         \n",
       "13  12     13     12_reseller_agreement_ehave_touchstar      "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model_cpu_1.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "156b27b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('party', 0.03041899480135238),\n",
       " ('agreement', 0.029226079792114903),\n",
       " ('shall', 0.024473584735500518),\n",
       " ('company', 0.02307833617389313),\n",
       " ('contract', 0.02116111833557621),\n",
       " ('agent', 0.015861823085333314),\n",
       " ('parties', 0.015733141752975032),\n",
       " ('related', 0.015681218009116288),\n",
       " ('the', 0.014880079515670825),\n",
       " ('details', 0.01455191449561325)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model_cpu_1.get_topic(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382f89a0",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69ea443",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=pd.DataFrame()\n",
    "results['dataset'] = ['news_dataset', 'CUAD_dataset','Amazon_dataset']\n",
    "results['BERTopic_time'] = ['end_time_cpu_0', 'end_time_cpu_1','end_time_cpu_2']\n",
    "results.to_csv('./results/overall_results_cpu.csv', index = False)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0febe4bc",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd23aff4-7ed1-4f16-989e-72522b4989cb",
   "metadata": {},
   "source": [
    "## Running customized BERTopic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bdc1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Customizing HDBSCAN \n",
    "hdbscan_model_1 = HDBSCAN(min_cluster_size=10, metric='euclidean', \n",
    "                        cluster_selection_method='eom', prediction_data=True, min_samples=5)\n",
    "topic_model_1 = BERTopic(hdbscan_model=hdbscan_model_1)\n",
    "topics_cpu_1, probs_cpu_1 = topic_model_1.fit_transform(data_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea57763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Customizng embedding\n",
    "from sentence_transformers import SentenceTransformer\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe3aa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Customizing embedding \n",
    "embeddings = sentence_model.encode(docs, show_progress_bar=False)\n",
    "topic_model_2 = BERTopic()\n",
    "topics_cpu_2, probs_cpu_2 = topic_model_2.fit_transform(data_0, embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
